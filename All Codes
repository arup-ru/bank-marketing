
#########################   "Analysis of a Portugese Bank Marketing Dataset"   #########################

install.packages("VIM")
install.packages("mice")
install.packages("caret")
install.packages("ROCR")
install.packages("randomForest")
install.packages("party")
install.packages("e1071")
install.packages("graphics")


# original dataset in bankA
# working dataset  in bankB (unknown <- NA; age <- age_group)
# age_group added  in bankC (22nd Column: age_group)



#########################                        #########################
          #####           Bank marketing DATASET               #####
#########################                        #########################


# Load Data in Data Frame
bankA <- as.data.frame(read.csv("C:/Users/arup.roy/Documents/bank-marketing-master/bankAdd.csv", sep= ";",header = T))

# Display the variables and first 10 records
str(bankA)
head(bankA,10)


# Replace all 'unknown' values with NA
bankB<-bankA
bankB[bankB=="unknown"]<-NA

summary(bankB$age)
#Min 17 #Max 98 #Mean 40 #Median 38


# Dividing the People into Different Age Groups
for(i in 1 : nrow(bankB)){
  if (bankB$age[i] <= 19){bankB$age_group[i] = '10s & Under'} 
  else if (bankB$age[i] >= 20 & bankB$age[i] <= 29){bankB$age_group[i] = '20s'} 
  else if (bankB$age[i] >= 30 & bankB$age[i] <= 39){bankB$age_group[i] = '30s'} 
  else if (bankB$age[i] >= 40 & bankB$age[i] <= 49){bankB$age_group[i] = '40s'}
  else if (bankB$age[i] >= 50 & bankB$age[i] <= 59){bankB$age_group[i] = '50s'}
  else if (bankB$age[i] >= 60 & bankB$age[i] <= 69){bankB$age_group[i] = '60s'}
  else if (bankB$age[i] >= 70 ){bankB$age_group[i] = '70s & Above'}
}

# saving the data before replacing age_group with age
bankC<-bankB 

bankB$age<-bankB$age_group
bankB<-bankB[1:21]
bankB$age<-as.factor(bankB$age)


# Separating New Customers from the Old ones
oldCust <- subset(bankB, bankB$poutcome != "nonexistent") 
summary(oldCust) 
#05625 Old Customers

newCust <- subset(bankB, bankB$poutcome == "nonexistent") 
summary(newCust) 
#35563 New Customers



#########################                       #########################
#####         All Customer DATASET          #####
#########################                       #########################


allCust <- bankB

# Missing value Frequencies
library(VIM) 

aggrPlot <- aggr(allCust, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#default 0.1022 #education 0.0480 #housing 0.0247 #loan 0.0247 #job 0.0065 #marital 0.0032

#Subscription Count
allCount <- table(allCust$y)
barplot(allCount,col=c("red","darkgreen"),legend = rownames(allCount), main = "All_Subscriptions")
#no 4126 #yes 1499 


# Impute Missing Values and Check
library(mice)

allCust2 <- mice(allCust)
allCust_com <- complete(allCust2)

aggrPlot <- aggr(allCust_com, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#none


#Split data into Train and Test subsets
library(caret)
set.seed(100)

allCust_com$y<-ifelse(allCust_com$y =='no', 0,1)
allCust_com$y<-as.factor(allCust_com$y)

ids <- sample(seq(1, 2), size = nrow(allCust_com), replace = TRUE, prob = c(.7, .3))

allCust_train <- allCust_com[ids==1,]
allCust_test  <- allCust_com[ids==2,]


table(allCust_train$y) #no 25714 #yes 3272
table(allCust_test$y)  #no 10834 #yes 1368

#########################                       #########################
            #####         Old Customer DATASET          #####
#########################                       #########################


# Missing value Frequencies
library(VIM) 

aggrPlot <- aggr(oldCust, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#default 0.1022 #education 0.0480 #housing 0.0247 #loan 0.0247 #job 0.0065 #marital 0.0032

#Subscription Count
oldCount <- table(oldCust$y)
barplot(oldCount,col=c("red","darkgreen"),legend = rownames(oldCount), main = "Old_Subscriptions")
#no 4126 #yes 1499 


# Impute Missing Values and Check
library(mice)

oldCust2 <- mice(oldCust)
oldCust_com <- complete(oldCust2)

aggrPlot <- aggr(oldCust_com, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#none


#Split data into Train and Test subsets
library(caret)
set.seed(101)

oldCust_com$y<-ifelse(oldCust_com$y =='no', 0,1)
oldCust_com$y<-as.factor(oldCust_com$y)

ids <- sample(seq(1, 2), size = nrow(oldCust_com), replace = TRUE, prob = c(.7, .3))

oldCust_train <- oldCust_com[ids==1,]
oldCust_test  <- oldCust_com[ids==2,]


table(oldCust_train$y) #no 2886 #yes 1027
table(oldCust_test$y)  #no 1240 #yes 472



#########################   Logistic Model (oldCust)   #########################


oldCust_logit <- glm(y ~., family=binomial(link='logit'), data = oldCust_train)
summary(oldCust_logit)

oldCust_logitResult <- predict(oldCust_logit, newdata=oldCust_test, type='response')
oldCust_logitResult <- ifelse(oldCust_logitResult >= 0.5,1,0)
oldCust_logitError  <- mean(oldCust_logitResult != oldCust_test$y)

print(paste('Accuracy for Logistic Model (oldCust)',1-oldCust_logitError))
#Accuracy = 84.29%

library(ROCR)

oldCust_logitPred <- prediction(oldCust_logitResult, oldCust_test$y)
oldCust_logitPerf <- performance(oldCust_logitPred, measure = "tpr", x.measure = "fpr")
plot(oldCust_logitPerf)

oldCust_logitAUC <- performance(oldCust_logitPred, measure = "auc")
oldCust_logitAUC <- oldCust_logitAUC@y.values[[1]]

print(paste('Area under the Curve for Logistic Model (oldCust)',oldCust_logitAUC))
#Area under Curve = 78.43%



#########################   Random Forest Model (oldCust)   #########################


library(randomForest)

oldCust_rf<-randomForest(y ~.,data = oldCust_train, importance=TRUE, ntree=1000)

oldCust_rfResult <- predict(oldCust_rf, oldCust_test)
oldCust_rfError  <- mean(oldCust_rfResult != oldCust_test$y)

print(paste('Accuracy for Random Forest Model (oldCust)',1-oldCust_rfError))
#Accuracy = 85.05%

library(ROCR)

oldCust_rfPred <- prediction(as.numeric(oldCust_rfResult), as.numeric(oldCust_test$y))
oldCust_rfPerf <- performance(oldCust_rfPred, measure = "tpr", x.measure = "fpr", acc = TRUE)
plot(oldCust_rfPerf)

oldCust_rfAUC <- performance(oldCust_rfPred, measure = "auc")
oldCust_rfAUC <- oldCust_rfAUC@y.values[[1]]

print(paste('Area under the Curve for Random Forest Model (oldCust)',oldCust_rfAUC))
#Area under Curve = 80.23%



#########################   Tree Model (oldCust)   #########################


library(party)

oldCust_tree<-ctree(y ~.,data = oldCust_train)
plot(oldCust_tree)

oldCust_treeResult <- predict(oldCust_tree, oldCust_test)
oldCust_treeError  <- mean(oldCust_treeResult != oldCust_test$y)

print(paste('Accuracy for Tree Model (oldCust)',1-oldCust_treeError))
#Accuracy = 84.23%

library(ROCR)

oldCust_treePred <- prediction(as.numeric(oldCust_treeResult), as.numeric(oldCust_test$y))
oldCust_treePerf <- performance(oldCust_treePred, measure = "tpr", x.measure = "fpr")
plot(oldCust_treePerf)

oldCust_treeAUC <- performance(oldCust_treePred, measure = "auc")
oldCust_treeAUC <- oldCust_treeAUC@y.values[[1]]
print(paste('Area under the Curve for Tree Model (oldCust)',oldCust_treeAUC))
#Area under Curve = 77.04%



#########################   Naive Bayes Model (oldCust)   #########################


library(e1071)

oldCust_nb<-naiveBayes(y ~.,data = oldCust_train)

oldCust_nbResult <- predict(oldCust_nb, oldCust_test)
oldCust_nbError  <- mean(oldCust_nbResult != oldCust_test$y)
print(paste('Accuracy for Naive Bayes Model (oldCust)',1-oldCust_nbError))
#Accuracy = 78.56%

library(ROCR)

oldCust_nbPred <- prediction(as.numeric(oldCust_nbResult), as.numeric(oldCust_test$y))
oldCust_nbPerf <- performance(oldCust_nbPred, measure = "tpr", x.measure = "fpr")
plot(oldCust_nbPerf)

oldCust_nbAUC <- performance(oldCust_nbPred, measure = "auc")
oldCust_nbAUC <- oldCust_nbAUC@y.values[[1]]
print(paste('Area under the Curve for Naive Bayes Model (oldCust)',oldCust_nbAUC))
#Area under Curve = 79.03%



#########################                      #########################
            #####         New Customer DATASET          #####
#########################                      #########################


# Since they are the new customers, it does not make sense to know 
# their outcome from the previous campaign, 
# number of previous contacts, 
# amount of day passed from their last contact and 
# their default credit with the bank

newCust$poutcome <-NULL
newCust$previous <-NULL
newCust$pdays    <-NULL
newCust$default  <-NULL


# Missing value Frequencies
library(VIM) 

aggrPlot <- aggr(newCust, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#education 0.0411 #housing 0.0239 #loan 0.0239 #job 0.0082 #marital 0.0017

#Subscription Count
newCount <- table(newCust$y)
barplot(newCount,col=c("red","darkgreen"),legend = rownames(newCount), main = "New_Subscriptions")
#no 32422 #yes 3141 


# Impute Missing Values and Check
library(mice)

newCust2 <- mice(newCust)
newCust_com <- complete(newCust2)

aggrPlot <- aggr(newCust_com, col=c('darkgreen','red'), ylab=c("Missing Value Histogram","Pattern"))
#none


#Split data into Train and Test subsets
library(caret)
set.seed(102)

newCust_com$y<-ifelse(newCust_com$y =='no', 0,1)
newCust_com$y<-as.factor(newCust_com$y)

id <- sample(seq(1, 2), size = nrow(newCust_com), replace = TRUE, prob = c(.7, .3))

newCust_train <- newCust_com[id==1,]
newCust_test  <- newCust_com[id==2,]

table(newCust_train$y) #no 22745 #yes 2191
table(newCust_test$y)  #no 9677  #yes 950

newCust_trains<- newCust_train 
# saving train data before making the model



#########################   Logistic Model (newCust)   #########################


newCust_logit <- glm(y ~., family=binomial(link='logit'), data = newCust_train)
summary(newCust_logit)

newCust_logitResult <- predict(newCust_logit, newdata=newCust_test, type='response')
newCust_logitResult <- ifelse(newCust_logitResult >= 0.5,1,0)
newCust_logitError  <- mean(newCust_logitResult != newCust_test$y)

print(paste('Accuracy for Logistic Model (newCust)',1-newCust_logitError))
#Accuracy = 92.01%

library(ROCR)

newCust_logitPred <- prediction(newCust_logitResult, newCust_test$y)
newCust_logitPerf <- performance(newCust_logitPred, measure = "tpr", x.measure = "fpr")
plot(newCust_logitPerf)

newCust_logitAUC <- performance(newCust_logitPred, measure = "auc")
newCust_logitAUC <- newCust_logitAUC@y.values[[1]]

print(paste('Area under the Curve for Logistic Model (newCust)',newCust_logitAUC))
#Area under Curve = 64.99%



#########################   Random Forest Model (newCust)   #########################


library(randomForest)

newCust_rf<-randomForest(y ~.,data = newCust_train, importance=TRUE, ntree=1000)

newCust_rfResult <- predict(newCust_rf, newCust_test)
newCust_rfError  <- mean(newCust_rfResult != newCust_test$y)

print(paste('Accuracy for Random Forest Model (newCust)',1-newCust_rfError))
#Accuracy = 91.85%

library(ROCR)

newCust_rfPred <- prediction(as.numeric(newCust_rfResult), as.numeric(newCust_test$y))
newCust_rfPerf <- performance(newCust_rfPred, measure = "tpr", x.measure = "fpr")
plot(newCust_rfPerf)

newCust_rfAUC <- performance(newCust_rfPred, measure = "auc")
newCust_rfAUC <- newCust_rfAUC@y.values[[1]]

print(paste('Area under the Curve for Random Forest Model (newCust)',newCust_rfAUC))
#Area under Curve = 70.04%



#########################   Tree Model (newCust)   #########################


library(party)

newCust_tree<-ctree(y ~.,data = newCust_train)
plot(newCust_tree)

newCust_treeResult <- predict(newCust_tree, newCust_test)
newCust_treeError  <- mean(newCust_treeResult != newCust_test$y)

print(paste('Accuracy for Tree Model (newCust)',1-newCust_treeError))
#Accuracy = 92.11%

library(ROCR)

newCust_treePred <- prediction(as.numeric(newCust_treeResult), as.numeric(newCust_test$y))
newCust_treePerf <- performance(newCust_treePred, measure = "tpr", x.measure = "fpr")
plot(newCust_treePerf)

newCust_treeAUC <- performance(newCust_treePred, measure = "auc")
newCust_treeAUC <- newCust_treeAUC@y.values[[1]]
print(paste('Area under the Curve for Tree Model (newCust)',newCust_treeAUC))
#Area under Curve = 76.54%



#########################   Naive Bayes Model (newCust)   #########################


library(e1071)

newCust_nb<-naiveBayes(y ~.,data = newCust_train)

newCust_nbResult <- predict(newCust_nb, newCust_test)
newCust_nbError  <- mean(newCust_nbResult != newCust_test$y)
print(paste('Accuracy for Naive Bayes Model (newCust)',1-newCust_nbError))
#Accuracy = 78.09%

library(ROCR)

newCust_nbPred <- prediction(as.numeric(newCust_nbResult), as.numeric(newCust_test$y))
newCust_nbPerf <- performance(newCust_nbPred, measure = "tpr", x.measure = "fpr")
plot(newCust_nbPerf)

newCust_nbAUC <- performance(newCust_nbPred, measure = "auc")
newCust_nbAUC <- newCust_nbAUC@y.values[[1]]
print(paste('Area under the Curve for Naive Bayes Model (newCust)',newCust_nbAUC))
#Area under Curve = 76.07%




#########################                      #########################                      #########################
               #####         OTHER ANALYSIS          #####                  OTHER ANALYSIS          #####
#########################                      #########################                      #########################

library(graphics)
par(mar=c(1,1,1,1))

# pcSuccess return the success rate of particular "x" variable from "ds" dataset
pcSuccess<-function(ds,x)
{
  barplot(100* summary(ds[which(ds$y == 1),][,x]) / summary(ds[,x]))
  return (100* summary(ds[which(ds$y == 1),][,x]) / summary(ds[,x]))
}

#age
pcSuccess(allCust_com,1)
pcSuccess(oldCust_com,1)
pcSuccess(newCust_com,1)

#job
pcSuccess(allCust_com,2)
pcSuccess(oldCust_com,2)
pcSuccess(newCust_com,2)

#marital
pcSuccess(allCust_com,3)
pcSuccess(oldCust_com,3)
pcSuccess(newCust_com,3)

#education (default)
pcSuccess(allCust_com,4)
pcSuccess(oldCust_com,4)
pcSuccess(newCust_com,4)

#housing
pcSuccess(allCust_com,6)
pcSuccess(oldCust_com,6)
pcSuccess(newCust_com,5)

#loan
pcSuccess(allCust_com,7)
pcSuccess(oldCust_com,7)
pcSuccess(newCust_com,6)

#contact
pcSuccess(allCust_com,8)
pcSuccess(oldCust_com,8)
pcSuccess(newCust_com,7)

#month
pcSuccess(allCust_com,9)
pcSuccess(oldCust_com,9)
pcSuccess(newCust_com,8)

#day_of_week
pcSuccess(allCust_com,10)
pcSuccess(oldCust_com,10)
pcSuccess(newCust_com,9)


#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~


modelResult<-function(model_train, model_test)
{
  model_logit       <- glm(y ~., family=binomial(link='logit'), data = model_train)
  model_logitResult <- predict(model_logit, newdata=model_test, type='response')
  model_logitResult <- ifelse(model_logitResult >= 0.5,1,0)
  model_logitError  <- mean(model_logitResult != model_test$y)
  
  model_logitPred <- prediction(model_logitResult, model_test$y)
  model_logitPerf <- performance(model_logitPred, measure = "tpr", x.measure = "fpr")
  model_logitAUC  <- performance(model_logitPred, measure = "auc")
  model_logitAUC  <- model_logitAUC@y.values[[1]]
  
  #~~~~~~~~~~~~~~~~~~~
  
  model_rf       <-randomForest(y ~.,data = model_train, importance=TRUE, ntree=1000)
  model_rfResult <- predict(model_rf, model_test)
  model_rfError  <- mean(model_rfResult != model_test$y)
  
  model_rfPred <- prediction(as.numeric(model_rfResult), as.numeric(model_test$y))
  model_rfPerf <- performance(model_rfPred, measure = "tpr", x.measure = "fpr")
  model_rfAUC  <- performance(model_rfPred, measure = "auc")
  model_rfAUC  <- model_rfAUC@y.values[[1]]
  
  #~~~~~~~~~~~~~~~~~~~
  
  model_tree       <-ctree(y ~.,data = model_train)
  model_treeResult <- predict(model_tree, model_test)
  model_treeError  <- mean(model_treeResult != model_test$y)
  
  model_treePred <- prediction(as.numeric(model_treeResult), as.numeric(model_test$y))
  model_treePerf <- performance(model_treePred, measure = "tpr", x.measure = "fpr")
  model_treeAUC  <- performance(model_treePred, measure = "auc")
  model_treeAUC  <- model_treeAUC@y.values[[1]]
  
  #~~~~~~~~~~~~~~~~~~~
  
  model_nb       <-naiveBayes(y ~.,data = model_train)
  model_nbResult <- predict(model_nb, model_test)
  model_nbError  <- mean(model_nbResult != model_test$y)
  
  model_nbPred <- prediction(as.numeric(model_nbResult), as.numeric(model_test$y))
  model_nbPerf <- performance(model_nbPred, measure = "tpr", x.measure = "fpr")
  model_nbAUC  <- performance(model_nbPred, measure = "auc")
  model_nbAUC  <- model_nbAUC@y.values[[1]]
  
  #~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#
  
  plot(model_logitPerf           , lwd=2, col = "blue")
  plot(model_rfPerf,   add = TRUE, lwd=2, col = "green")
  plot(model_treePerf, add = TRUE, lwd=2, col = "orange")
  plot(model_nbPerf,   add = TRUE, lwd=2, col = "magenta")
  
  prt_logit <-paste('Logistic Regression (AUC =',          round (100*model_logitAUC,2), "%)")
  ptr_rf    <-paste('Random Forest         (AUC =',        round (100*model_rfAUC,2),    "%)")
  ptr_tree  <-paste('Binary Tree                 (AUC =',  round (100*model_treeAUC,2),  "%)")
  ptr_nb    <-paste('Naive Bayes               (AUC =',    round (100*model_nbAUC,2),    "%)")
  
  legend("bottomright", legend=c(prt_logit, ptr_rf, ptr_tree, ptr_nb), col=c("blue", "green", "orange", "magenta"), lwd=2, cex = .75)
  
  print(paste('Accuracy of Logistic Regression Model:', round (100*(1-model_logitError),2), "%"))
  print(paste('Accuracy of Random Forest Model:      ', round (100*(1-model_rfError),2),    "%"))
  print(paste('Accuracy of Binary Tree Model:        ', round (100*(1-model_treeError),2),  "%"))
  print(paste('Accuracy of Naive Bayes Model:        ', round (100*(1-model_nbError),2),    "%"))
  
}

library(randomForest)
library(party)
library(e1071)

modelResult(allCust_train, allCust_test)
modelResult(oldCust_train, oldCust_test)
modelResult(newCust_train, newCust_test)


  #~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~
  #~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~



modelResult<-function(model_train, model_test)
{
  model_logit       <- glm(y ~., family=binomial(link='logit'), data = model_train)
  model_logitResult <- predict(model_logit, newdata=model_test, type='response')
  model_logitResult <- ifelse(model_logitResult >= 0.5,1,0)
  model_logitError  <- mean(model_logitResult != model_test$y)
  
  model_logitPred <- prediction(model_logitResult, model_test$y)
  model_logitPerf <- performance(model_logitPred, measure = "tpr", x.measure = "fpr")
  model_logitAUC  <- performance(model_logitPred, measure = "auc")
  model_logitAUC  <- model_logitAUC@y.values[[1]]
  
  #~~~~~~~~~~~~~~~~~~~
  
  model_rf       <-randomForest(y ~.,data = model_train, importance=TRUE, ntree=1000)
  model_rfResult <- predict(model_rf, model_test)
  model_rfError  <- mean(model_rfResult != model_test$y)
  
  model_rfPred <- prediction(as.numeric(model_rfResult), as.numeric(model_test$y))
  model_rfPerf <- performance(model_rfPred, measure = "tpr", x.measure = "fpr")
  model_rfAUC  <- performance(model_rfPred, measure = "auc")
  model_rfAUC  <- model_rfAUC@y.values[[1]]

  #~~~~~~~~~~~~~~~~~~~
  
  model_tree       <-ctree(y ~.,data = model_train)
  model_treeResult <- predict(model_tree, model_test)
  model_treeError  <- mean(model_treeResult != model_test$y)

  model_treePred <- prediction(as.numeric(model_treeResult), as.numeric(model_test$y))
  model_treePerf <- performance(model_treePred, measure = "tpr", x.measure = "fpr")
  model_treeAUC  <- performance(model_treePred, measure = "auc")
  model_treeAUC  <- model_treeAUC@y.values[[1]]

  #~~~~~~~~~~~~~~~~~~~
  
  model_nb       <-naiveBayes(y ~.,data = model_train)
  model_nbResult <- predict(model_nb, model_test)
  model_nbError  <- mean(model_nbResult != model_test$y)
  
  model_nbPred <- prediction(as.numeric(model_nbResult), as.numeric(model_test$y))
  model_nbPerf <- performance(model_nbPred, measure = "tpr", x.measure = "fpr")
  model_nbAUC  <- performance(model_nbPred, measure = "auc")
  model_nbAUC  <- model_nbAUC@y.values[[1]]

  #~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~#~~~~~~~~~~~~~~~~~~~
  
  plot(model_logitPerf           , lwd=2, col = "blue")
  plot(model_rfPerf,   add = TRUE, lwd=2, col = "green")
  plot(model_treePerf, add = TRUE, lwd=2, col = "orange")
  plot(model_nbPerf,   add = TRUE, lwd=2, col = "magenta")
  
  prt_logit <-paste('Logistic Regression (AUC =',          round (100*model_logitAUC,2), "%)")
  ptr_rf    <-paste('Random Forest         (AUC =',        round (100*model_rfAUC,2),    "%)")
  ptr_tree  <-paste('Binary Tree                 (AUC =',  round (100*model_treeAUC,2),  "%)")
  ptr_nb    <-paste('Naive Bayes               (AUC =',    round (100*model_nbAUC,2),    "%)")
  
  legend("bottomright", legend=c(prt_logit, ptr_rf, ptr_tree, ptr_nb), col=c("blue", "green", "orange", "magenta"), lwd=2, cex = .75)
  
  print(paste('Accuracy of Logistic Regression Model:', round (100*(1-model_logitError),2), "%"))
  print(paste('Accuracy of Random Forest Model:      ', round (100*(1-model_rfError),2),    "%"))
  print(paste('Accuracy of Binary Tree Model:        ', round (100*(1-model_treeError),2),  "%"))
  print(paste('Accuracy of Naive Bayes Model:        ', round (100*(1-model_nbError),2),    "%"))
  
}


modelResult(allCust_train, allCust_test)






pkgs <- c("factoextra",  "NbClust")
install.packages(pkgs)

library(factoextra)
# library(NbClust)

df <- scale(USArrests)
head(df)

oldBank_num <- data.frame(as.numeric(as.factor(oldCust_com$age)),
                          as.numeric(as.factor(oldCust_com$job)),
                          as.numeric(as.factor(oldCust_com$marital)),
                          as.numeric(as.factor(oldCust_com$education)),
                          as.numeric(as.factor(oldCust_com$housing)),
                          as.numeric(as.factor(oldCust_com$loan)),
                          as.numeric(as.factor(oldCust_com$y)))

newBank_num <- data.frame(as.numeric(as.factor(newCust_com$age)),
                          as.numeric(as.factor(newCust_com$job)),
                          as.numeric(as.factor(newCust_com$marital)),
                          as.numeric(as.factor(newCust_com$education)),
                          as.numeric(as.factor(newCust_com$housing)),
                          as.numeric(as.factor(newCust_com$loan)),
                          as.numeric(as.factor(newCust_com$y)))

allBank_num <- data.frame(as.numeric(as.factor(allCust_com$age)),
                          as.numeric(as.factor(allCust_com$job)),
                          as.numeric(as.factor(allCust_com$marital)),
                          as.numeric(as.factor(allCust_com$education)),
                          as.numeric(as.factor(allCust_com$housing)),
                          as.numeric(as.factor(allCust_com$loan)),
                          as.numeric(as.factor(allCust_com$y)))

BankAdditionalNum<-oldBank_num
colnames(BankAdditionalNum) <- c("Age", "Job", "Marital", "Education", "Housing", "Loan", "y")

par(mfrow=c(1,1))

fviz_nbclust(BankAdditionalNum, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

fviz_nbclust(BankAdditionalNum, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method")

fviz_nbclust(BankAdditionalNum, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")


k2 <- kmeans(BankAdditionalNum, centers = 2, nstart = 25); k2
k3 <- kmeans(BankAdditionalNum, centers = 3, nstart = 25); k3
k4 <- kmeans(BankAdditionalNum, centers = 4, nstart = 25); k4


km<-function(ds)
{
  colnames(ds) <- c("Age", "Job", "Marital", "Education", "Housing", "Loan", "y")
  
  sh <- fviz_nbclust(ds, kmeans, method = "silhouette")+
    labs(subtitle = "Silhouette method")
  
  # ws <- fviz_nbclust(ds, kmeans, method = "wss") +
  #   geom_vline(xintercept = 2, linetype = 2)+
  #   labs(subtitle = "Elbow method")
  
  # fviz_nbclust(ds, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  #   labs(subtitle = "Gap statistic method")
  
  k2 <- kmeans(ds, centers = 2, nstart = 25) 
  k3 <- kmeans(ds, centers = 3, nstart = 25)
  k4 <- kmeans(ds, centers = 4, nstart = 25)
  
  kList<- list(sh, ws, k2, k3, k4)
  return(kList)
}

km(allBank_num)



